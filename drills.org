#+STARTUP: latexpreview
* Math / Stats
** Basic Rules of Probability
*** Drills
**** Additivity                                                    :drill:
$P(A \vee B) = \ ?$
***** Answer
$P(A \vee B) = P(A) + P(B) - P(A \wedge B)$
**** Conditional Probability                                       :drill:
$P(A|B) = \ ?$

Do not answer with Bayes' rule. The answer should contain $P(A \wedge B)$.
***** Answer
$P(A|B) = \frac{P(A \wedge B)}{P(B)}$
**** Multiplication                                                :drill:
$P(A \wedge B) = \ ?$
***** Answer
$P(A \wedge B) = P(A|B)P(B)$
**** Total Probability                                             :drill:
How is $P(A)$ related to $B$ ?
***** Answer
$P(A) = P(A|B)P(B) + P(A| \neg B)P( \neg B)$
**** Bayes' Rule                                                   :drill:
What is Bayes' rule?
***** Answer
$P(A|B) = \frac{ P(B|A)P(A) }{ \sum_{i} P(B|A_i)P(A_i) } = \frac{ P(B|A)P(A) }{ P(B) }$
**** Standard Deviation of Population                              :drill:
What is the standard deviation for a population?
***** Answer
$\sqrt{\sum_{i} [(X_i - X_A) ^ 2] / n}$ where $X_A$ is the average for the population.
**** Standard Deviation Given P and N                              :drill:
What is the standard deviation if given $p$ and $n$ ?
***** Answer
$\sigma = \sqrt{(1-p)pn}$
**** Worst Case Confidence Interval                                :drill:
What's a good rule of thumb for the worst case confidence interval?
***** Answer
Use $\sigma = \sqrt{(1-p)pn}$ where $p = 0.5$, because when $p = 0.5$, $\sigma$ is maximized for any given $n$.
**** Three Sigma Rule                                              :drill:
What is the "three sigma rule"? (Hint: It's related to standard deviations.)
***** Answer
"68% - 95% - 99.7%"

These are the percentages of values that lie within 1, 2, and 3 standard deviations of a mean.
** Greek Alphabet
*** Drills
**** gamma                                                         :drill:
What is this Greek letter? $\gamma$
***** Answer
Lowercase gamma
**** Gamma                                                         :drill:
What is this Greek letter? $\Gamma$
***** Answer
Uppercase gamma
**** delta                                                         :drill:
What is this Greek letter? $\delta$
***** Answer
Lowercase delta
**** Delta                                                         :drill:
What is this Greek letter? $\Delta$
***** Answer
Uppercase delta
**** epsilon                                                       :drill:
What is this Greek letter? $\epsilon$
***** Answer
Lowercase epsilon
**** theta                                                         :drill:
What is this Greek letter? $\theta$
***** Answer
Lowercase theta
**** Theta                                                         :drill:
What is this Greek letter? $\Theta$
***** Answer
Uppercase theta
**** lambda                                                        :drill:
What is this Greek letter? $\lambda$
***** Answer
Lowercase lambda
**** Lambda                                                        :drill:
What is this Greek letter? $\Lambda$
***** Answer
Uppercase lambda
**** mu                                                            :drill:
What is this Greek letter? $\mu$
***** Answer
Lowercase mu
**** pi                                                            :drill:
What is this Greek letter? $\pi$
***** Answer
Lowercase pi
**** Pi                                                            :drill:
What is this Greek letter? $\Pi$
***** Answer
Uppercase pi
**** rho                                                           :drill:
What is this Greek letter? $\rho$
***** Answer
Lowercase rho
**** sigma                                                         :drill:
What is this Greek letter? $\sigma$
***** Answer
Lowercase sigma
**** Sigma                                                         :drill:
What is this Greek letter? $\Sigma$
***** Answer
Uppercase sigma
**** phi                                                           :drill:
What is this Greek letter? $\phi$
***** Answer
Lowercase phi
**** Phi                                                           :drill:
What is this Greek letter? $\Phi$
***** Answer
Uppercase phi
**** psi                                                           :drill:
What is this Greek letter? $\psi$
***** Answer
Lowercase psi
**** Psi                                                           :drill:
What is this Greek letter? $\Psi$
***** Answer
Uppercase psi
**** omega                                                         :drill:
What is this Greek letter? $\omega$
***** Answer
Lowercase omega
**** Omega                                                         :drill:
What is this Greek letter? $\Omega$
***** Answer
Uppercase omega
** Bayesian Analysis
*** Drills
**** Normal Distribution                                           :drill:
What is the normal distribution formula?
***** Answer
$p(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{(-\frac{1}{2}[\frac{x-\mu}{\sigma}]^2)$

Where $\mu$ is the mean (average), and $\sigma$ is the standard deviation.
**** Variance                                                      :drill:
What is variance? How does it relate to the standard deviation?
***** Answer
$variance = \sigma^2 = \frac{\sum_i{(X_i - \mu)^2}}{N}$

$standard\ deviation = \sqrt{variance}$

In other words, variance is the average squared difference (or "mean squared error"). The standard deviation is the square root of the variance.
**** Deriving Bayes' Rule                                          :drill:
Derive Bayes' rule from $p(a|b) = \frac{p(b,a)}{p(b)}$
***** Answer
Multiply both sides by $p(b)$: $p(a|b) = \frac{p(b,a)}{p(b)} \implies p(b,a) = p(a|b)p(b)$

"and" is commutative: $p(a,b) = p(b,a)$

Substitute the long form of $p(a,b)$ and $p(b,a)$: $p(b|a)p(a) = p(a|b)p(b)$

Divide both sides by $p(b)$: $\frac{p(b|a)p(a)}{p(b)} = p(a|b)$

That is Bayes' rule. Remember that $p(b) = \sum_{a_i} p(b|a_i)p(a_i)$
**** Parts of Bayes' Rule                                          :drill:
What are the four parts of Bayes' rule?
***** Answer
$p(a|b) = \frac{p(b|a)p(a)}{p(b)}$

$p(a|b)$ is the "posterior".

$p(b|a)$ is the "likelihood".

$p(a)$ is the "prior".

$p(b)$ is the "evidence".
**** Beta Distribution                                             :drill:
What is the Beta distribution?
***** Answer
$beta(x|a,b) = \frac{x^{(a-1)}(1-x)^{(b-1)}}{\int_0^1 x^{(a-1)}(1-x)^{(b-1)}\ dx}$

The denominator is a "normalizing constant" which causes the probability function to sum to 1.

Some guidelines for choosing a and b:

$mean = \mu = a/(a+b)$

$mode = \omega = (a-1)/(a+b-2)$

$concentration = k = a + b$

Concentration is the number of data points needed to equally balance the prior.
**** Metropolis Algorithm                                          :drill:
What is the Metropolis algorithm?
***** Answer
It is a "random walk", or Monte Carlo Markov Chain (MCMC) algorithm, for sampling a distribution.

The algorithm is:

1) Keep track of the "current position" in the distribution and all previously visited positions.

2) Generate a proposed move to another position. There are various ways to do this, some are more effective than others.

3) Move from the current position to the proposed position with probability:

$p(move) = min(1, \frac{d(proposed)}{d(current)})$

Where $d$ is the distribution we are sampling.

4) In an infinitely long run, the history of visited positions will be a perfect sample from the distribution.

With the right parameters, a finitely long run will be a good sample from the distribution.
